{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006911,
     "end_time": "2020-09-01T12:37:11.757833",
     "exception": false,
     "start_time": "2020-09-01T12:37:11.750922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-09-01T12:37:32.697209Z",
     "iopub.status.busy": "2020-09-01T12:37:32.696476Z",
     "iopub.status.idle": "2020-09-01T12:37:32.732707Z",
     "shell.execute_reply": "2020-09-01T12:37:32.732216Z"
    },
    "papermill": {
     "duration": 0.051001,
     "end_time": "2020-09-01T12:37:32.732806",
     "exception": false,
     "start_time": "2020-09-01T12:37:32.681805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc ,random \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "\n",
    "import cv2\n",
    "import SimpleITK as sitk\n",
    "from ipywidgets import interact, fixed\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import albumentations as A \n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from loss.dice import * \n",
    "from loss.ssim import * \n",
    "from models.UNet import *\n",
    "from datasets.merging_dataset import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-01T12:37:32.756492Z",
     "iopub.status.busy": "2020-09-01T12:37:32.755808Z",
     "iopub.status.idle": "2020-09-01T12:37:32.759377Z",
     "shell.execute_reply": "2020-09-01T12:37:32.759913Z"
    },
    "papermill": {
     "duration": 0.021453,
     "end_time": "2020-09-01T12:37:32.760024",
     "exception": false,
     "start_time": "2020-09-01T12:37:32.738571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SEED Everything \n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn (img1, img2):\n",
    "    return 1-SSIM()(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    \n",
    "    pred =  torch.sigmoid(pred)\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    loss_label_1 = loss[:,0].mean()\n",
    "    loss_label_2 = loss[:,1].mean()\n",
    "    loss_label_3 = loss[:,2].mean()\n",
    "    loss_label_4 = loss[:,3].mean()\n",
    "\n",
    "    return ((loss_label_1+loss_label_2+loss_label_3+loss_label_4)/4 , (loss_label_1 , loss_label_2 ,loss_label_3,loss_label_4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006019,
     "end_time": "2020-09-01T12:37:32.854121",
     "exception": false,
     "start_time": "2020-09-01T12:37:32.848102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_fn(data_loader, model, optimizer, scheduler,device):\n",
    "    model.train()\n",
    "    tr_loss = 0 \n",
    "    counter = 0 \n",
    "    if verbose : \n",
    "        tk0 = tqdm( enumerate(data_loader) , total= len(data_loader) )\n",
    "    else : \n",
    "        tk0 = enumerate(data_loader)\n",
    "    for bi, d in tk0 : # LOOP : batch number i   \n",
    "        real_mask = d[\"label\"].to(device, dtype=torch.long)\n",
    "        LR = d[\"LR\"].to(device, dtype=torch.float) \n",
    "        HR = d[\"HR\"].to(device, dtype=torch.float)  \n",
    "        \n",
    "        HR_1 , mask_1 , HR_2 , mask_2 , HR_3 , mask_3 = model(LR.unsqueeze(1)) #forward prop\n",
    "        \n",
    "        loss_seg_1 , _ = dice_loss (mask_1, real_mask) # Loss calaculation of batch i \n",
    "        loss_seg_2 , _ = dice_loss(mask_2 , real_mask)\n",
    "        loss_seg_3 , _ = dice_loss(mask_3 , real_mask)\n",
    "        \n",
    "        loss_rec_1 = loss_fn(HR_1, HR.unsqueeze(1) )\n",
    "        loss_rec_2 = loss_fn(HR_2 , HR.unsqueeze(1) )\n",
    "        loss_rec_3 = loss_fn(HR_3 , HR.unsqueeze(1) )\n",
    "        \n",
    "        loss_segxrec = loss_fn(HR_1 * real_mask , HR.unsqueeze(1) * real_mask)\n",
    "        loss = (loss_seg_1+loss_seg_2+loss_rec_1+loss_rec_2+loss_seg_3+loss_seg_3)/6 + loss_segxrec\n",
    "        optimizer.zero_grad() #\n",
    "       \n",
    "        tr_loss += loss.item()\n",
    "        counter +=1 \n",
    "        \n",
    "        loss.backward()  # backward prop \n",
    "        optimizer.step() \n",
    "    return tr_loss/counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_fn(data_loader, model , device ):\n",
    "    model.eval()\n",
    "    seg_loss = 0 \n",
    "    rec_loss = 0\n",
    "    counter = 0\n",
    "    \n",
    "    label1_loss  = 0\n",
    "    label2_loss  = 0 \n",
    "    label3_loss  = 0 \n",
    "    label4_loss = 4 \n",
    "    if verbose : \n",
    "        tk0 = tqdm( enumerate(data_loader) , total= len(data_loader) )\n",
    "    else : \n",
    "        tk0 = enumerate(data_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for bi, d in tk0 :\n",
    "       \n",
    "            real_mask = d[\"label\"].to(device, dtype=torch.long)\n",
    "\n",
    "            LR = d[\"LR\"].to(device, dtype=torch.float) \n",
    "            HR = d[\"HR\"].to(device, dtype=torch.float)  \n",
    "            HR_1 , mask_1 , _ , _ , _ , _   = model(LR.unsqueeze(1)) #forward prop\n",
    "            \n",
    "            loss , labels = dice_loss(mask_1, real_mask) # Loss calaculation of batch i \n",
    "            \n",
    "            ssim_score = loss_fn(HR_1 , HR.unsqueeze(1) )\n",
    "            \n",
    "            label1_loss += labels[0].item() \n",
    "            label2_loss += labels[1].item() \n",
    "            label3_loss += labels[2].item() \n",
    "            label4_loss += labels[3].item() \n",
    "            \n",
    "            seg_loss += loss.item()\n",
    "            rec_loss += ssim_score.item()\n",
    "            counter +=1 \n",
    "\n",
    "        \n",
    "            \n",
    "        return rec_loss/counter , seg_loss/counter ,  (label1_loss /counter ,label2_loss /counter, label3_loss /counter,label4_loss/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-01T12:37:32.908162Z",
     "iopub.status.busy": "2020-09-01T12:37:32.892584Z",
     "iopub.status.idle": "2020-09-01T12:37:32.928948Z",
     "shell.execute_reply": "2020-09-01T12:37:32.928402Z"
    },
    "papermill": {
     "duration": 0.050281,
     "end_time": "2020-09-01T12:37:32.929036",
     "exception": false,
     "start_time": "2020-09-01T12:37:32.878755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(model, EPOCHS , train_dataset , valid_dataset , device , LR , TRAIN_BATCH_SIZE ,VALID_BATCH_SIZE):\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle = True , \n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=8\n",
    "    )\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "    num_train_steps = int(len(train_data_loader)) * EPOCHS\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)   \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "    train_loss =  []\n",
    "    rec_score = []\n",
    "    seg_score = []\n",
    "    val_loss = []\n",
    "    best_validation_rec = 5.0\n",
    "    best_validation_seg = 5.0\n",
    "    patience = 0 \n",
    "    for epoch in range(EPOCHS):\n",
    "        if verbose : \n",
    "            print(f'--------- Epoch {epoch} ---------')\n",
    "        elif epoch%10==0 : \n",
    "            print(f'--------- Epoch {epoch} ---------')\n",
    "        tr_loss=train_fn(train_data_loader, model, optimizer, scheduler,device)\n",
    "        train_loss.append(tr_loss)\n",
    "        if verbose : \n",
    "            print(f\" train_loss  = {tr_loss}\")\n",
    "        elif epoch%10==0 : \n",
    "            print(f\" train_loss  = {tr_loss}\")\n",
    "        rec , seg , _  = eval_fn(valid_data_loader, model,device)\n",
    "        rec_score.append(1-rec)\n",
    "        seg_score.append(1-seg)\n",
    "        if verbose : \n",
    "            print(f\" Segmentation  Dice  = {1-seg} , Reconstruction SSIM = {1-rec}\")\n",
    "        elif epoch%10==0 : \n",
    "            print(f\" Segmentation  Dice  = {1-seg} , Reconstruction SSIM = {1-rec}\")\n",
    "        if rec < best_validation_rec : \n",
    "            best_validation_rec =rec \n",
    "            patience = 0 \n",
    "            torch.save(model.state_dict(), 'recPCUNetV2.pt')\n",
    "        elif seg  < best_validation_seg : \n",
    "            best_validation_seg = seg \n",
    "            patience = 0 \n",
    "            torch.save(model.state_dict(), 'segPCUNetV2.pt')\n",
    "        else : \n",
    "            patience +=1\n",
    "        if patience>20 : \n",
    "            print(f'Eraly Stopping on Epoch {epoch}')\n",
    "            print(f'Best Loss Seg =  {best_validation_seg}')\n",
    "            print(f'Best Loss Rec =  {best_validation_rec}')\n",
    "            break\n",
    "        scheduler.step()\n",
    "    return rec_score,seg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005854,
     "end_time": "2020-09-01T12:37:32.940672",
     "exception": false,
     "start_time": "2020-09-01T12:37:32.934818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Left hippocampal Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-01T12:37:32.963333Z",
     "iopub.status.busy": "2020-09-01T12:37:32.962527Z",
     "iopub.status.idle": "2020-09-01T12:37:32.973871Z",
     "shell.execute_reply": "2020-09-01T12:37:32.973288Z"
    },
    "papermill": {
     "duration": 0.027267,
     "end_time": "2020-09-01T12:37:32.973986",
     "exception": false,
     "start_time": "2020-09-01T12:37:32.946719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParallelCascadedUNet(nn.Module) : \n",
    "    def __init__(self) :\n",
    "        super(ParallelCascadedUNet,self).__init__()\n",
    "        \n",
    "        self.segmentation_1  = UNet(1,4, segmentation = False) \n",
    "        self.segmentation_2  = UNet(2,4, segmentation = False)\n",
    "        self.segmentation_3  = UNet(2,4, segmentation = False)\n",
    "        \n",
    "        self.reconstruction_1 = UNet(1,1, segmentation = False)\n",
    "        self.reconstruction_2 = UNet(5,1, segmentation = False)\n",
    "        self.reconstruction_3 = UNet(5,1, segmentation = False)\n",
    "    \n",
    "    def forward(self,image) :\n",
    "        seg_1 = self.segmentation_1(image) \n",
    "        rec_1 = self.reconstruction_1(image)\n",
    "        \n",
    "        x_hr_lr = torch.cat([rec_1,image] , dim =1)\n",
    "        seg_2  = self.segmentation_2(x_hr_lr) \n",
    "        \n",
    "        c_seg = self.construct_seg(seg_1)\n",
    "        x_merge = torch.cat([image,c_seg] , dim = 1 )\n",
    "        rec_2 = self.reconstruction_2(x_merge)\n",
    "        \n",
    "        x_hr_lr = torch.cat([rec_2,image] , dim =1)\n",
    "        seg_3  = self.segmentation_3(x_hr_lr) \n",
    "        \n",
    "        c_seg = self.construct_seg(seg_2)\n",
    "        x_merge = torch.cat([image,c_seg] , dim = 1 )\n",
    "        rec_3 = self.reconstruction_3(x_merge)\n",
    "        \n",
    "        return rec_3,seg_3,rec_2 , seg_2 , rec_1 , seg_1 \n",
    "    def construct_seg(self,x) : \n",
    "        y_1 = torch.argmax(nn.Softmax2d()(x) , dim=1)\n",
    "        x_label_0 = (y_1==0).type(torch.long).unsqueeze(1)\n",
    "        x_label_1 = (y_1==1).type(torch.long).unsqueeze(1)\n",
    "        x_label_2 = (y_1==2).type(torch.long).unsqueeze(1)\n",
    "        x_label_3 = (y_1==3).type(torch.long).unsqueeze(1)\n",
    "        generated_segmenation = torch.cat([x_label_0,x_label_1,x_label_2,x_label_3] , dim = 1)\n",
    "        generated_segmenation = generated_segmenation.type(torch.float)\n",
    "        return generated_segmenation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('data_5fold.csv')\n",
    "subjects = all_data[all_data['slice']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-01T12:37:32.988933Z",
     "iopub.status.busy": "2020-09-01T12:37:32.988084Z",
     "iopub.status.idle": "2020-09-01T12:37:32.999249Z",
     "shell.execute_reply": "2020-09-01T12:37:33.000012Z"
    },
    "papermill": {
     "duration": 0.020268,
     "end_time": "2020-09-01T12:37:33.000125",
     "exception": false,
     "start_time": "2020-09-01T12:37:32.979857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 32\n",
    "LR = 3e-4\n",
    "EPOCHS = 250\n",
    "device = torch.device('cuda')\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-01T12:37:33.021636Z",
     "iopub.status.busy": "2020-09-01T12:37:33.020750Z",
     "iopub.status.idle": "2020-09-01T12:37:33.031331Z",
     "shell.execute_reply": "2020-09-01T12:37:33.030815Z"
    },
    "papermill": {
     "duration": 0.025021,
     "end_time": "2020-09-01T12:37:33.031433",
     "exception": false,
     "start_time": "2020-09-01T12:37:33.006412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_training_augmentation]  resize_to: (160, 160)\n",
      "--------- Epoch 0 ---------\n",
      " train_loss  = 0.8702474665641785\n",
      " Segmentation  Dice  = 0.2202843594551086 , Reconstruction SSIM = 0.5536249852180481\n",
      "--------- Epoch 10 ---------\n",
      " train_loss  = 0.4964650774002075\n",
      " Segmentation  Dice  = 0.46595941305160526 , Reconstruction SSIM = 0.7341284656524658\n",
      "--------- Epoch 20 ---------\n",
      " train_loss  = 0.4143490648269653\n",
      " Segmentation  Dice  = 0.5890400695800782 , Reconstruction SSIM = 0.7444382119178772\n",
      "--------- Epoch 30 ---------\n",
      " train_loss  = 0.2169656726717949\n",
      " Segmentation  Dice  = 0.8579121768474579 , Reconstruction SSIM = 0.7484765434265137\n",
      "--------- Epoch 40 ---------\n",
      " train_loss  = 0.18827419996261596\n",
      " Segmentation  Dice  = 0.9071718965470791 , Reconstruction SSIM = 0.7496367359161378\n",
      "--------- Epoch 50 ---------\n",
      " train_loss  = 0.17787837117910385\n",
      " Segmentation  Dice  = 0.9047674818336964 , Reconstruction SSIM = 0.7518754911422729\n",
      "--------- Epoch 60 ---------\n",
      " train_loss  = 0.17922213047742844\n",
      " Segmentation  Dice  = 0.8936246553063393 , Reconstruction SSIM = 0.752426929473877\n",
      "--------- Epoch 70 ---------\n",
      " train_loss  = 0.17102083534002305\n",
      " Segmentation  Dice  = 0.9082339003682136 , Reconstruction SSIM = 0.7536242437362671\n",
      "--------- Epoch 80 ---------\n",
      " train_loss  = 0.17435143947601317\n"
     ]
    }
   ],
   "source": [
    "train_folds_loss = []\n",
    "valid_folds_loss = []\n",
    "for f in range(5) : \n",
    "    df_train = all_data[all_data['kfold'] !=f]\n",
    "    df_valid = all_data[all_data['kfold'] ==f]\n",
    "    Left_train_dataset = Merging_data_set(df_train ,  subjects  , Left = True , is_train = True)\n",
    "    Left_valid_dataset = Merging_data_set(df_valid  ,  subjects  , Left = True , is_train  = False)\n",
    "    Left_model = ParallelCascadedUNet()\n",
    "    Left_model = Left_model.to(device)\n",
    "    val_loss , train_loss =  run( Left_model  , EPOCHS , Left_train_dataset , Left_valid_dataset , device , LR , TRAIN_BATCH_SIZE , VALID_BATCH_SIZE )\n",
    "    train_folds_loss.append(train_loss)\n",
    "    valid_folds_loss.append(val_loss) \n",
    "    \n",
    "    Left_model.load_state_dict(torch.load('recPCUNetV2.pt'), strict=False)\n",
    "    torch.save(Left_model.state_dict(), f'trained_model/PCasceded UNet/Rec PCascaded Unet V2 Left fold {f}.pt')\n",
    "    \n",
    "    Left_model.load_state_dict(torch.load('segPCUNetV2.pt'), strict=False)\n",
    "    torch.save(Left_model.state_dict(), f'trained_model/PCasceded UNet/Seg PCascaded Unet V2 Left fold {f}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-01T12:37:33.049846Z",
     "iopub.status.busy": "2020-09-01T12:37:33.049083Z",
     "iopub.status.idle": "2020-09-01T12:37:33.051645Z",
     "shell.execute_reply": "2020-09-01T12:37:33.052269Z"
    },
    "papermill": {
     "duration": 0.014847,
     "end_time": "2020-09-01T12:37:33.052377",
     "exception": false,
     "start_time": "2020-09-01T12:37:33.037530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_(f) :\n",
    "    plt.plot(train_folds_loss[f]) \n",
    "    plt.plot(valid_folds_loss[f])\n",
    "    plt.title(f'Learning curve fold 0={f}')\n",
    "    plt.ylabel('score')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 25.592187,
   "end_time": "2020-09-01T12:37:33.256544",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-01T12:37:07.664357",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
